{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le fichier CSV\n",
    "df = pd.read_csv(r\"C:/Users/antoa/Desktop/projet certif/app/rag_csv.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pas de valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index           0\n",
       "url             0\n",
       "nom_produit     0\n",
       "img_produit     0\n",
       "prix_produit    0\n",
       "contenance      0\n",
       "pg_vg           0\n",
       "origine         0\n",
       "frais           0\n",
       "surbooste       0\n",
       "saveur          0\n",
       "description     0\n",
       "brand           0\n",
       "gout            0\n",
       "info_brand      0\n",
       "id_produit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tester l'intégrité des embeddings\n",
    "Avant d'utiliser les embeddings (sentence-transformers/all-MiniLM-L6-v2), il faut tester s'ils génèrent bien des vecteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antoa\\anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\antoa\\anaconda3\\envs\\nlp\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Test passé : Embeddings fonctionnels.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "def test_embeddings():\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    text = \"Test embedding\"\n",
    "    embedding = model.encode(text)\n",
    "\n",
    "    assert isinstance(embedding, np.ndarray), \"L'embedding n'est pas un numpy array\"  #embedding est bien un objet de type numpy.ndarray (un tableau numpy)\n",
    "    assert embedding.shape[0] > 0, \"L'embedding est vide\" # La dimension du tableau (le nombre d'éléments dans l'embedding) est supérieure à 0, ce qui garantit que l'embedding n'est pas vide.\n",
    "    print(\"Test passé : Embeddings fonctionnels.\")\n",
    "\n",
    "test_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur : La colonne 'texte' ou 'content' est absente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Charger le modèle\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Fonction pour tester les embeddings\n",
    "def test_embeddings_from_csv(csv_file):\n",
    "    # Charger le CSV\n",
    "    df = pd.read_csv(csv_file, encoding=\"utf-8\")\n",
    "\n",
    "    # Supposons que la colonne contenant les textes s'appelle 'texte' ou 'content'\n",
    "    if 'texte' not in df.columns and 'content' not in df.columns:\n",
    "        print(\"Erreur : La colonne 'texte' ou 'content' est absente.\")\n",
    "        return\n",
    "\n",
    "    # Extraire les textes\n",
    "    texts = df['texte'] if 'texte' in df.columns else df['content']\n",
    "\n",
    "    # Tester les embeddings pour chaque texte\n",
    "    for i, text in enumerate(texts):\n",
    "        embedding = model.encode(text)\n",
    "\n",
    "        # Vérifications des assertions\n",
    "        assert isinstance(embedding, np.ndarray), f\"L'embedding à la ligne {i+1} n'est pas un numpy array\"\n",
    "        assert embedding.shape[0] > 0, f\"L'embedding à la ligne {i+1} est vide\"\n",
    "        print(f\"Test passé pour la ligne {i+1}: Embedding fonctionnel.\")\n",
    "\n",
    "# Appeler la fonction pour tester le CSV\n",
    "test_embeddings_from_csv(r\"C:/Users/antoa/Desktop/projet certif/app/rag_csv.csv\")\n",
    "# df = pd.read_csv(r\"C:/Users/antoa/Desktop/projet certif/app/rag_csv.csv\", encoding=\"utf-8\")\n",
    "# ../RGBD/table_produits/produits.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from app.embeddings import embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\antoa\\Desktop\\projet certif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from config import Config\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=Config.CSV_FILE_PATH, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Chargement des documents\n",
    "def load_documents(file_path):\n",
    "    try:\n",
    "        loader = CSVLoader(file_path=Config.CSV_FILE_PATH, encoding='utf-8')\n",
    "        documents = loader.load()\n",
    "        print(f\"Nombre de documents chargés : {len(documents)}\")\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des documents : {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Création et sauvegarde du vecteur\n",
    "def save_vectorstore(documents):\n",
    "    try:\n",
    "        vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "        vectorstore.save_local('faiss_vector_store')\n",
    "        print(\"Vectorstore sauvegardé avec succès.\")\n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la création du vectorstore : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Chargement du vecteur\n",
    "def load_vectorstore():\n",
    "    try:\n",
    "        vectorstore = FAISS.load_local('faiss_vector_store', embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "        print(\"Vectorstore chargé avec succès.\")\n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement du vectorstore : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tester la recherche dans le vectorstore\n",
    "def test_search_in_vectorstore(query):\n",
    "    try:\n",
    "        vectorstore = load_vectorstore()\n",
    "        if vectorstore:\n",
    "            retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.1})\n",
    "            results = retriever.retrieve(query)\n",
    "            print(f\"Résultats de la recherche : {len(results)} document(s) trouvé(s).\")\n",
    "            return results\n",
    "        else:\n",
    "            print(\"Le vectorstore n'a pas pu être chargé.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la recherche : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_search_in_vectorstore(query):\n",
    "    try:\n",
    "        vectorstore = load_vectorstore()\n",
    "        if vectorstore:\n",
    "            retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.1})\n",
    "            # Utilisation de la méthode 'get_relevant_documents' pour récupérer les documents\n",
    "            results = retriever.get_relevant_documents(query)  # Changer de 'retrieve' à 'get_relevant_documents'\n",
    "            print(f\"Résultats de la recherche : {len(results)} document(s) trouvé(s).\")\n",
    "            return results\n",
    "        else:\n",
    "            print(\"Le vectorstore n'a pas pu être chargé.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la recherche : {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processus complet de test\n",
    "def test_vectorstore(file_path, query):\n",
    "    documents = load_documents(file_path)  # 1. Charger les documents\n",
    "    if documents:\n",
    "        save_vectorstore(documents)  # 2. Sauvegarder le vectorstore\n",
    "        test_search_in_vectorstore(query)  # 4. Tester la recherche\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de documents chargés : 100\n",
      "Vectorstore sauvegardé avec succès.\n",
      "Vectorstore chargé avec succès.\n",
      "Résultats de la recherche : 4 document(s) trouvé(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antoa\\AppData\\Local\\Temp\\ipykernel_18456\\3343200202.py:7: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(query)  # Changer de 'retrieve' à 'get_relevant_documents'\n"
     ]
    }
   ],
   "source": [
    "# Tester avec ton fichier CSV et une requête d'exemple\n",
    "file_path = 'rag_csv.csv'\n",
    "query = \"Exemple de requête pour rechercher un document\"\n",
    "test_vectorstore(file_path, query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
